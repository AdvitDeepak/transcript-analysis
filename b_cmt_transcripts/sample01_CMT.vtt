1. Andrew Knight. 00:00:04.859 -> 00:00:21.240

Okay, so we're recording. We're streaming
We have that setup. Okay.
It's like Mary Kate's here. I'll let her in.
Gonna make her a host to

5. Paul McCartney. 00:00:24.000 -> 00:00:26.070

She just got back from a vacation or something.

6. George Harrison. 00:00:27.330 -> 00:00:31.110

Mary Kate.
I think she was on a vacation for a while.

8. Ringo Starr. 00:00:31.650 -> 00:00:33.630

Oh, nice. See how that one.

9. Andrew Knight. 00:00:37.560 -> 00:00:38.100

America, Kate.

10. John Lennon. 00:00:39.480 -> 00:00:43.950

Hi, how's it going good.
It's everything good to go.

12. George Harrison. 00:00:44.790 -> 00:00:45.990

Thanks. So great.

13. Andrew Knight. 00:00:46.740 -> 00:00:53.070

Yeah, I can start letting people in if that's
That's useful. Yeah.

15. John Lennon. 00:00:53.760 -> 00:01:04.200

Whatever you whatever is easiest for you.
So pop up screen that I saw why I said I liked the pop up screen that I saw before I got in. I had seen that before.

17. Andrew Knight. 00:01:04.860 -> 00:01:05.370

What was it

18. John Lennon. 00:01:05.490 -> 00:01:10.260

It said host is waiting to let you in. And it said like the name of the event.

19. Andrew Knight. 00:01:11.400 -> 00:01:36.570

Okay.
So we will. I'll do just a brief.
Brief very brief intro because I baked. A lot of it into the pre set slide that will circulate throughout. So I'll just do a brief very brief intro before I intro you throw it over to you and you can share your screen then and and dive in. Awesome. Thanks. Okay.

22. Paul McCartney. 00:01:36.780 -> 00:01:48.630

Do you need to look at that. That select part of a screen for your PowerPoint again.
I think I did. Let me just try share screen. Yeah.
It's not good.

25. John Lennon. 00:01:50.220 -> 00:01:50.670

Yeah.

26. Paul McCartney. 00:01:51.750 -> 00:01:53.790

And it works. You're seeing it move

27. John Lennon. 00:01:54.690 -> 00:01:57.270

Yep, yep. It looks great there you

28. Paul McCartney. 00:01:57.450 -> 00:01:58.740

Go back to the original

29. Andrew Knight. 00:02:05.670 -> 00:02:15.960

All right, Mary Kate, if anything goes off, I'll be watching the chat if you just text me or like enter anything into the chat there that needs to

30. Paul McCartney. 00:02:16.110 -> 00:02:21.540

Be adjusted
Like private someone cell phone, just in case.
Something goes horribly wrong.

33. Andrew Knight. 00:02:22.440 -> 00:02:28.890

Yep. Oh. Enter mine in a direct private chat, just so that it's always there for you. Thanks.

34. OMITTED. 00:02:30.060 -> 00:02:30.420

And that

35. Andrew Knight. 00:02:35.340 -> 00:02:36.660

And I have that sitting here.

36. Paul McCartney. 00:02:36.960 -> 00:02:45.630

Too. I'm just gonna call that number so that you have it 321-584-0624. Awesome. Thanks.

37. Andrew Knight. 00:02:46.500 -> 00:02:47.640

So were you the 917

38. OMITTED. 00:02:48.000 -> 00:02:49.320

Yes. Okay.

39. Andrew Knight. 00:02:50.670 -> 00:13:55.740

Alright, so I'm gonna let people in
Hello everyone. Welcome, welcome to our program this afternoon or this evening or this morning, depending on where you're joining us from
You are more than welcome. Of course to unmute your video if you want to join the session and have a presence here with us if that's possible. It's always a wonderful way to build the community.
My name is Andrew night. I'm a professor of organizational behavior in Olin business school. And I'm also the academic director of lifelong learning.
And if there's one thing I've learned so far from online teaching it is that there is enormous value in having the community that you're a part of right there in front on the screen. And so if you aren't too shy to broadcast your video, please do so.
We have an outstanding program ahead. And so I just want to take a couple of minutes here at the outset.
To share with you how this program fits into some of the goals and the work that we're doing.
for lifelong learning and so let me just share a couple of slides at the outset, before we welcome Professor liberty visitors who is going to be
Sharing her expertise and knowledge with us today. As you may or may not know Olin lifelong learning is an initiative.
That is specifically focused on providing opportunities for alumni to engage in continuous development and the rationale for this initiative.
Is very much the fact that we make a promise in our vision and our mission statement to provide a return on investment for alumni over the course of their entire careers. And so that's what we are.
Working on with the Lifelong Learning Initiative, which is one quadrant of olins always Olin initiative.
As you've seen in some of the slides at the outset we are building currently
And testing currently a platform that will support alumni learning in community. And so this is a platform that's going to be used to deliver content, similar to what we're going to experience today, but also other
events that happen on campus, other kinds of thought leadership from our faculty as well as from alumni. And so this platform is going to be steadily rolling out in the weeks and months that come
You'll be able to find archived content things that have happened at the school in the past that we think are relevant.
And there will be opportunities for you to engage with your fellow alumni around some of those pieces of content so that you're sharing your expertise wisdom and insights with one another.
Now as I mentioned at the start, we are so fortunate today to have with us. Professor liberty visitors.
Who is a Professor of Practice of data science here in the business school and she is someone who truly practices. What she preaches
She is active in disseminating her knowledge and expertise with respect to statistics data and analytics through so many different channels.
Whether that is through the popular media where you might encounter some of her work and commentary, or through doing things like writing a book which is coming out here on the horizon.
Just this month. And so we are so fortunate to have someone who is skilled and has deep expertise in this topic, and at the same time someone who is a skilled communicator about this topic. And so without further ado, please join me in welcoming Professor liberty that hurt.

61. Paul McCartney. 00:13:57.180 -> 00:52:49.770

Hi, everybody. Thank you. I think I need to fire my PR person. And have you, Andrew. That was great. Um, does the screen. Look. Okay. Can you all see it.
I think I hope I shared it right I'm learning zoom. Okay. Um, thank you all so much for having me here today I wanted to sort of
dive right in. I have these two graphs or two if you get bored listening to me talk
But, um, you know, we've lived as humanists in our decision making for a very long time we go off our thoughts or opinions, our experience our gut and we are now living in this era of data ism.
Where we have data drive our decisions, you know, from climate change to health care the refugee crisis. These all encompassing issues.
But also in our everyday lives. You know, I don't go to the bookstore anymore. Amazon tells me what to read or dating apps tell you who you're compatible with
And I read an article like two or three days ago from new research that red wine is terrible for you and I swear. Two days later I read that red wine is wonderful for you and you should drink it.
And it feels almost that humanism and data ism, or sort of pushing back against each other right now. Some people want to still go with that gut instinct and others want to have the drive are every decision.
But really the future is about bringing them together and that's what we're going to sort of touch on and have a taste of today is how we can get incredible ideas from data.
But we also need to use that human touch. And I also want to say please use common questions, anything you want in the chat and makes it a lot more fun.
Throughout the whole thing and I'll respond as we go. Um, so I want to start before we go into our many examples. And by the way, all these examples are short. So if one's boring. Just wait a couple minutes moving to a new one.
Um, I wanted to visit with a French monk from the 17th century Blaise Pascal. You might also remember them from high school without Pascal's triangle so Pascal was a French monk who had a crisis of faith, but he decided to analyze his data.
He felt that he had four scenarios. First, God doesn't exist, but he chooses to believe in God. He may live a wasted life with false belief, but when he dies, nothing happens.
Now, he could not believe in God and God doesn't exist, and he may not have lived a wasted life of false belief, but again, when he dies, nothing happens.
Now the third option God exists, and he believes in God, when he dies, he has the best case scenario heaven for him.
But if God exists, and we don't believe in God. The fourth scenario, then it's eternal Hellfire really bad thing.
So what Pascal decided to do was to minimize the risk of the bad thing happening by believing in God and maximizing the potential is a good thing.
So he ruled out this possibility and left a 5050 chance of the best thing happening.
And that's really what data science is it's taking information from past and current events to predict the likelihood of a future event happening really the closest thing to a crystal ball. But the world has
The only difference between us and Pascal is we don't have four scenarios or four pieces of information we have endless amounts of data that is telling us what to do.
And we also have something else. We're not sitting in a monastery, while we do this, we are having governments, businesses companies managers customers, clients even scientists, researchers
Trying to inform manipulate or persuade us using this data. So we need to be able to ask the right questions when data comes to us from a report or study or anything comes to us.
I want to discuss the questions that I ask every time I see one of these things. So we're going to have our for questions. And we're going to have examples of how if someone had just asked those questions.
Everything would have been averted. The downside would have been minimized and the upside maximized. So let's start with our first question, who
OJ Simpson, one of the most infamous criminal trials in American history to remind you. Oh Jay was an NFL superstar accused of murdering his ex WIFE NICOLE SIMPSON AND her lover Ronald while Goodman. I'm sure all of a lot of you remember the
Case, and I'm sure a lot of you who are a little bit younger might remember one the Kardashian father is one of his lawyers, just to try to give some parallels there.
Um, but, Alan Dershowitz whatever your personal feelings of him might be a fate is a famed legal mind in America, and he made a very interesting statistical argument during the trial.
He said that while it was known and definite that OJ beat Nicole absolutely no contest. So the fact that he physically abused her that it didn't matter.
Because 4 million women in the United States are abused by their domestic partners, every year, but only one in 2500 of them are then murdered by their domestic partner. So the chance that OJ did it is one in 2500 and you can't convict someone on a one in 2500 chains.
However, what the jury and most certainly the prosecution should have realized is that the statistic that Mr Dershowitz stated was completely irrelevant and here's why and Mr Dershowitz situation we have a living women who are currently domestic victims of domestic abuse.
But at the time of the trial. NICOLE SIMPSON WAS NOT A living woman who's a victim of domestic abuse, whose chance of dying is one in 2500
But she's a murdered woman who was a victim of domestic abuse. We don't want to know the statistic about living women. We want to know the statistic about murdered women.
Who were victims of domestic abuse. And the question we want to know is that if you were a murdered woman.
Who was a victim of domestic abuse, what is the chance that the guy who abused you murdered you versus being the victim of some random homicide.
And in that case, the chance is nine and 10 so the chance that OJ killed Nicole was actually 90% not one in 2500
Though who we really wanted to know this statistic about changed everything.
And it is so easy to miss the prosecution had their entire careers writing on this case, the whole world was watch it and they missed it.
This simple slip of the tongue of who we really want to know our data about living women who are victims murdered women who were changes everything. So that who question is integral whenever you get data.
Let's look at one more example of that Abraham Wald was a Jewish statistician who'd been locked out of the University positions and fled to the US before World War Two.
To avoid persecution and hungry.
And he was recruited to help with the Allied cause and look at plane bombers that came back, he was told the analyze the bullet wounds on the plane bombers that came back.
And decide which area of the plane that he should reinforce. So my question is, if you're Abraham walled, how do you decide by looking at these planes.
What area, you should reinforce if you reinforce 10% of the plane.
And he started with this data. This is the data. He had of all the planes that had come back bullet holes per square foot of the section of the plan. So just take one second, five seconds and think, where you would reinforce the plane. If you were Walt. Given this data.
Anyone is brave enough to put it into the chat. I'll be very impressed.
Real cockpit. Great.
Anywhere but we have everyone is too smart here.
Where the red dots are red gunner everyone is really smart here okay engines. So what most people think
And what most people say is the area where there are the most bullets. Right. So the wings, the fuselage where the dots are the areas most bullets. That is what most people think and the most obvious answer.
What Walt is so famous for doing is saying that's actually not where we put it where we need to reinforce the planes or the undamaged parts and here's why.
We have to who's we have planes that crashed abroad, the planes, the bombers that didn't make it back.
And we have the planes that wall does able to see that we have did trivia. Trivia, I just saw this question in the chat, maybe lose my train of thought the trivia that came back. Now the planes that returned home.
So we have two samples planes that crashed abroad planes that returned home, these are the ones come and see my cursor. I hope so. So I'm like pointing at the screen to which obviously you can see
The planes that returned home, or what Walt was able to see and those planes when they got hit on the wings or the cockpit.
Right here. There's the cockpit. This is our engine when they got hit in the wings. They did their job. They made it back.
They were not hit in the cockpit or the engine, the planes that crashed abroad, when they were hit in the cockpit or engine. They couldn't make it back.
So, those who were looking at, or sometimes ones we can't even see and it gives us a whole nother answer of where we should be reinforcing the planes, so that both groups, both samples are able to make it back.
So this brings us to our second question we have our first have who is the data really about, who do we want to know this data about and next
Is what exactly are we, why are we prone to making this type of mistake error. Very good question. Andrew
Part of what happens when we're trying to think about these things is we miss the simple answer right. Where's look so focused on looking at the data, the planes that made it back.
That we don't take a step back and say, Who is it that we really care about. We don't really care about the planes that made it back, right, because they made it back. They did their job.
We actually care about the who didn't make it back. So it's, it's all about taking one step back from the data and asking these actually very broad stroke questions.
So our second question is, what what is, what is it that we really want to know if our data.
Uber, there was an Uber ban in London Transport for London office. The authorizing authority for all taxis in London band Uber.
Now, there's a lot of questions about why exactly they did it. But why they said they did it. Is that Uber was dangerous Uber had become associated with sexual assaults. I remember I was in London.
I was getting an Uber. I was on the phone with my father and I said, Dad. I'll call you back. And he said, No, no, no. Liberty. Where are you, I said, I'm getting into Uber. He goes no, I've heard those are so dangerous in London. You can't get into what
It's like 4pm on a Tuesday. But anyway, the public believed that Uber was dangerous the public truly believed it. So how, how did the public come to believe that Uber was dangerous.
Well, the London taxi drivers Association, which is what you think of when you think of a black cab in London. They put up an advertisement.
Last year there are 154 rapes and sexual assaults by many cab drivers in London at least 32 of these Uber drivers.
There's also an image of a girl her makeup is streaming down her face. She is clearly just been sexually assaulted.
I just wanted to go home. Don't take a risk with a mini cap that's an incredibly powerful advertisement. And if you notice the Metropolitan Police. The London Police logo.
And the Transport for London office logo are both at the bottom of this ad incredibly powerful.
And just do us a little bit of vocab when they say taxis taxis are black cabs, which are what you think of when you think of a London cab and mini caps that's any other type of private car transportation
Uber's included so when they say 154 rapes and sexual assaults by mini cab drivers in London, they mean everything but blackcaps
Very powerful advertisement. The problem is that it wasn't true.
What the freedom of information request act actually said is that there were 154 allegations of rape or sexual assault, where the suspect was alleged to be a taxi driver that includes blackcaps 32 of these or by an Uber driver.
So clearly the black cab Association had taken themselves out of this when they were asked about this. They said, oops, sorry clerical error.
Oh, we'll take it down and they did and they put up another one last year there 32 allegations of rape or sexual assault made against Uber drivers, the same image of the girl her makeup streaming down her face. She has clearly just been sexually assaulted. I just wanted to go home.
Why up your risk with a mini cab. Now, it's true. The information is true. But, what question do we really want to know of our data.
Do you really up your risk. So let's take a look at the very simple numbers. We were given the wrong number of salts 32 by Uber drivers 154 total
But is that really what we want to know of our numbers. Is that really what we want to know of our data.
If Uber was responsible for 32 sexual assaults, but only did 32 journeys, the entire year in London, they would be very dangerous. There's 100% chance you'll be sexually assaulted. If you get an Uber.
What if they did millions of journeys. What if they did many more than all of the others. So let's take a look.
Proportion of sexual assaults by Uber drivers 32 by Uber 154 total. So they are Uber's responsible for 20% of a sexual assaults in London.
But how many journeys are they responsible for number of journeys in London and any kind of taxi, including Uber's 3,000,001 million of those are completed by Uber.
So Uber is responsible for 33% of the journeys, but only 20% of the sexual assaults, meaning that Uber is actually a safer way to get around than any other type of tax.
And what about these headlines, the number of cab drivers charged with violence or sexual offenses in London. Is it a five year high
Trying to say that with the introduction of Uber London has taxis have gotten much less safe well in 2015 there 136 reports of rape or sexual assault by taxis. When this headline was written.
And what is the definition of a five year high, it means it was just as high five years ago in 2009 three years before Uber even started there 136 reports of rape or sexual assault by taxi drivers Uber had nothing to do with making it more unsafe.
Now this seems like an unfortunate situation for Uber. But there's more to it. There's 3.5 million customers who last and arguably better product.
40,000 jobs are potentially lost from this and most Uber drivers and lease or contract higher their cars and then obviously the revenue for Uber in London.
So what we really want to know of our data, the raw numbers. The proportions, the relative proportions changes the entire analysis.
So now we'll do one more example of what that's a little bit lighter and my mother. My mother was very excited when she saw these headlines.
Chocolate can stave off all simers as cocoa, the brain drug of the future chocolate improves brain function. This is about a year and a half ago, my mother was ecstatic. She called me. She said, liberty, I told you so.
This is amazing. And she asked me the question that we probably wanted to ask of the data. How much chocolate. Can you look back at the study liberty. How much chocolate. Can I actually get to eat.
In order to potentially reduce my risk. So that was the question she asked, and that is what we want to know. Right. How much chocolate. Do I get to eat to potentially reduce my risk.
Well, it's obviously not chocolate. That's what reduces our risk. It's a bioactive called flavanols and flavanols are in chocolate. They're also in red wine.
And so the question really is, is how many flavanols. Do I have to eat every day. And that's 510 milligrams. So unless you're going to take a five and all pill. You want to know how many flavanols are in chocolate.
So let's say you, um, you're not really a chocolate person, but you might drink, eat some cocoa powder, you know, put it in, mix it into water a little hot chocolate.
And you'd have to have 20 cups of hot chocolate every day in order to potentially reduce your risk.
If you don't like hot chocolate and your dark chocolate person you'd have to have 1600 calories of dark chocolate every day.
And if you're like me and you like milk chocolate. That's 3900 calories every day of milk chocolate.
In order to potentially reduce your risk. Now, I'm not saying I couldn't do that. Okay, but I am saying that I would die of something else, way before I had to worry about getting Alzheimer's or dementia.
And yes, this as well as the previous example, I'm looking at the chat does have enormous really really fascinating correlations to health care and we're actually going to do one of those examples. Next, especially with Kobe that I think should be interesting for everyone to see.
So the question of what we really want to know what we care about death by chocolate yes 3900 calories a day. I think we could do it for a little while, but it may not be great.
And I also I, you know, when I was looking back at this study from my mom, I thought, you know, I wonder who funded
This study who wanted us to think that chocolate was so great for us. And it's amazing that Mars center for cocoa health science was in charge of the research for this one. They just, you know, so kind of them to want us to, you know, have their products do good for us very frightened.
So we get to our third question we have, who is the data. What do we really want to know, and we get to how is it interpreted
So french fries. I like chocolate. But I like French fries, a lot better and I saw this this headline come out eight year study
Finds heavy french fry eaters have double the chance of death could french fries be killing you french fries are killing you.
This is true. According to a peer reviewed study published in the American Journal of Clinical Nutrition and what the journal actually said
Is that if you eat any form of fried potato more than twice a week, which means three times per week or more you double your risk of death.
And it's true. That is what the journal said, and I went back to the study. So let's, let's start with our questions who well in the study. The average person was a 60 YEAR OLD MAN Okay.
And what we really want to know is, we want to know what the differences for risk of death of a 60 year old man, if you eat no potatoes. No french fries versus if you do
And that is doubling your risk of death. So our COO and our what makes sense. The question is, is how is it interpreted. So let's look at it a little bit of a different way than doubling your risk of death.
If you're a 60 year old man, your risk of death at any point in time, regardless of how much you eat.
Is 1% meaning that if I line up 160 year old men. One of them will die in the next year, just by virtue of the fact that they're 60 and their male sorry 60 year old man apologize but it's going to be better at the end.
Now, if these 100 men eat french fries three times per week or more for their entire lives instead of one of them dying.
To will we double our risk of death instead of one, two, which is still bad. Okay, but it sounds a whole lot better to go from a 1% to a 2% then doubling your risk of death.
And I also, you know, I don't know about you guys, but when I eat french
Like I eat french fries. I love them, but I don't really just eat french fries. Right. I have like a couple of beers. I have a big old juicy cheeseburger right there with me.
And you know if I'm eating french fries three times per week or more, I'm not sure I'm like eating perfectly healthy. The rest of the time. Okay.
I may not exactly be you know out there running five miles per day. If I'm eating french fries three times per week or more. So could it potentially be something else that has an effect on this.
So that how we interpret, whether it's double risk of death or one in 100 to two and 100 and these dudes get to eat french fries. All time
changes how we see this data. Let's look at another example. And this brings us to the covert testing that we were discussing here earlier.
And yes, it is too bad that we can't rely on news outlets, you really have to think about these things and ask these questions yourself, which is very, very difficult.
Okay, I have the antibodies. So it's safe. If we hang out. We now have an antibody test for coven
Meaning that if you've had coven you take the antibody test. If you have the antibodies. The idea is that you are then immune from coven and you also cannot pass it on to other people.
For the purposes of this will assume that's true that if you have antibodies. You can you assume that you're immune and you cannot pass it on to other people.
And I've had people say this to me. I've had friends say this to me. I have the antibodies. So it's safe. We can hang out. You can be around me and then go be around your elderly parents, no problem.
So the question is, is that true, how good is the testing. So the CDC put out guidance to the general public about these antibody tests. They said, and a population or the prevalence is 5% a test with 90% sensitivity and 95% specificity will yield the positive predictive value of 49%
Sorry. You think the general person that's taking an antibody test has any idea what that means include. I mean, it takes me and this is what I do for a living. And it takes me a while to sit down there and puzzle out what this means.
How are people supposed to really understand this. Yes, this is definitely where the headache comes on. Thank you.
So let's take a step back and think about what question we really want to know the first test approved by the FDA for coven 19 antibody testing as sent on the CDC website is 95% accurate.
What we want to know is what is the chance if your friend comes to you and says, I have the antibodies, it's safe to hang out.
What you want to know is what is the chance that an individual actually has the antibodies, assuming they test positive
Most people would assume based upon the CDC website is that it's 95% the test is 95% accurate. So there's a 95% chance that they have the antibodies and then it's fine.
Let's take a closer look. If you have an antibody test. There are four possible outcomes outcome. One is you are positive and you test positive the test did its job. You're all good.
Outcome to is that you are negative for antibodies and you test negative again the test did its job.
But because the test is not 100% accurate. There are two other possible outcomes. The first is a false negative
That means the test says you don't have the antibodies test negative. But you do have them the falls apart.
Now, that one's not that dangerous, right. It's kind of annoying because you wish you knew that you had the antibodies and you can go out and do stuff.
But you're not going to pass the disease on to, and you're going to pass the virus on to anyone else because you're going to think that you don't have the end the body. So you need to stay home.
But there's a fourth option, a false positive, the test says you have the antibodies, but you don't
And this is the really dangerous one. Because you're going to think that you're going to your immune, you're going to think you can't pass it on to anyone else. And you're going to go out into the wide world and pass it on.
So the question is, how often does this happen. So let's let's look. Let's take a real scenario.
5% of the population has coded 19 that's what the CDC meant when they said 5% prevalence, meaning that they 5% is had it and we'll test positive for it.
And the coven 19 antibody test is 95% accurate. So those are two assumptions and starting points. So let's say we have a population of thousand people. Let's say it's 1000 people at work.
So if there's 5% prevalence in the population 5% of these people have had coven and will test positive for it 5% of 1000 is 50 people so 50 people have antibodies and we'll test positive for tested its job.
But the test is only 95% accurate meaning 5% of the time, it won't be accurate.
So we will have a 5% false positive rate, people will falsely test positive, it will tell them that they have the antibodies, but they don't 5% of 1000 is again 50 so we have 50 people who will falsely test positive
Now out of our thousand people all 100 of these people will test positive all 100 old test positive, but only 50 of them actually have it.
Meaning there's only a 50% chance that you actually have the antibodies. So back to our original question of what is the chance to be an individual actually has the antibodies, assuming they test positive
50%. That's it. It's the same as a toss of a coin. So when we think that if we have the antibodies. We can go back out into the world.
There's a 5050 chance that it won't work. Yes. Not all 1000 people get the test that is true, but we're going, we're basing this idea of antibody testing on that it will help us get back to work.
And so we would test everyone in the population and how it, but people who test negative impact this style at all in general that number for this 95% accurate what they meant was 5% false positives and 5% false negatives. So there would also be a large issue with that as well.
Okey dokey.
So, how we interpret the data, you know, CDC put out that statement, but how we interpret it as 95% accurate versus actually 5050 changes the entire way that we look at the situation.
So now we get to our final example of how and my favorite one finished baby boxes in 2013 the BBC put out this article Why finished babies sleep in cardboard boxes.
It was an article documenting the finished policy of giving every mother a cardboard box baby supplies and the baby would use the box as a bed.
And this is a really interesting example of what happened because it exploded all over the place. People were talking about these baby boxes and for me, when my Molly, I will be right back to you. I'm going to write outside talk about that at the end.
But inside this article was a graph and it showed Finland's incredible decrease in infant mortality rate incredible progress from 1935 down here to 2010
And it also included Finland's and current infant mortality rate which is two and 1000 versus the rest of the world, which is 32 and 1000
So really incredible based upon this policy of sending every mother a baby box.
And it sparked an entire industry finished baby boxes British baby boxes. I actually went to a shower recently and the girl, my friend who was having a baby got two baby boxes at this whole industry.
And it's incredible really this box this cardboard box that can save a baby's life. It's not just any cardboard box. It's an $850 cardboard box.
I'm not really sure what they print on that thing. But inside of it actually looks got one to look at it and I'm inside. It has six diapers, which I'm I think any parents would be able to decide how long six diapers last
Like two one Z's. A couple books. So really, the, the whole thing is this cardboard box.
Is your baby slept one baby slept in a dresser drawer exactly same concept but you know i i would admit I hope the dresser Jordan cost 800 bucks, but you never know.
And so, but, you know, okay, if we really believed that this box would save your child's life would save any child's life, you'd say fine 800 bucks. No problem. You would do anything to get that box anything. So the question is, does it
Let's take a look back at this exact same graph and how we're going to interpret this data and how we're going to interpret this graph.
In 1938 Finland had a higher infant mortality profile, then it's other Nordic neighbors and it decided it needed to do something about it.
So it went on this incredible marketing campaign convincing others rich, poor educated or uneducated that they if they had their kids sleeping this baby box back kids slept in it they be fine. They'd survive, but this would save their child's life.
And when every mother was convinced of this and wanted this baby box. The finished government said, you know, there's actually a catch.
In order to get this baby box every mother will get one, but you have to go to prenatal education class.
So in 1944 the year before the box came out 31% finished and others receive Prenatal Education in 1945 the year after 86% of finished mothers received Prenatal Education. It wasn't the box at all. It was education and general progress of info mortality rates across the world.
And the reason we know this is because the infant mortality profile for rich, poor educated and uneducated finished mothers.
Is the same as the infant mortality profile for rich and educated mothers in the US, the difference is born on educated mothers in the US have a much higher infant mortality profile. Now you can forgive industry for
You know, deciding to profit and go, go forth and conquer off of these finished baby boxes.
But you would think governments and other governments would sort of look into this sort of how this these baby boxes really worked before embarking upon a scheme.
To send out baby boxes but no in a baby boxes were sent out in Essex in Birmingham and Scotland. They piloted a program in 2016 and now every single mother and Scotland receives a baby box. No. Prenatal Education required to the tune of I think $13 million of taxpayer money.
But you know, that's the UK. We're America or smarter our government would definitely look into this stuff before piloting these baby boxes.
Except they don't US states introduced baby boxes to help combat infant mortality in 2017 Alabama New Jersey and Ohio sent out baby boxes to the tune of 15,000,020 6,000,030 $5 million tons and tons of taxpayer money wasted with no Prenatal Education required
And if someone had simply asked how is this interpreted. How does, what does this graph really mean we wouldn't be here.
And as Alonso asks, How do you get to think to all the questions that are not intuitive to the data presented
I know these questions seems stupid who what and how but I am telling you they work if you just take a step back and think, Who is this data about
What are we really trying to ask of it and how should we really be interpreting this, it may not give you the answers. It may not make us. It's not going to make you a statistician
But it will give you a feeling for whether the veracity of this data really need something
And this this example leads us to our fourth question of why, why do we care and I want to, I want to use the same baby box numbers. And I want to say, you know,
I don't know the difference between 26,000,035 million except that it's 9 million
These numbers don't mean anything to me and our jobs and our work and newspapers, we see these numbers. Every time we see billions. We see trillions.
For a lot of states 15,000,035. These are rounding errors for states for companies. How do we have a we make these numbers mean something. Why should we care about this money that's lost
And I would say that going along the lines of anyone looking at this would be interested in sort of Child Health and baby health that we somehow figure out a way to make those numbers mean something in terms of that in terms of their opportunity cost as Mandy says
So how do we do that, I would say, then instead of saying that Alabama wasted $15 million of taxpayer money.
They could have paid for diapers for a year for 30,000 babies or New Jersey could have bought baby food for a year for 40,000 babies or Ohio could have paid a year salary for 800 new child social care workers.
We need to turn these numbers into something that matters to people into something that people understand and personally matters to them.
And that brings me to our last example you know we've talked this whole time about negative things you know the right you know
About the, you know, all the manipulative things bad things sneaky things. But there's also incredible things that we can do. There's really incredible things we can do with actually making people care about the numbers.
Or in the most severe refugee crisis since world war two and it's only growing UNHCR, the UN Refugee agencies budget for 2016 was $8
Billion, but they only got 3 billion. That's $5 billion needed to help people. So what do they do they need to figure out a way to raise more money.
That is, but as what needs to be done for this refugee crisis. So how do they do it, and I'm going to show you a very disturbing image. Next, so please. Just be ready for that.
This is Alan Curtis. He's a three year old Syrian boy who in 2015 was found drowned on a Turkish beach and his family were Syrian refugees fleeing to Europe.
This pictures unfathomably horrible event is really what sparked the world's discussion of the Syrian refugee crisis and one would imagine. I mean, how could you not look at that photo and feel sort of a deep on unbelievable sadness and it did increase donations, but not for very long.
Three days was how long this image stayed in people's minds. There's absolutely horrible, horrible tragedy, and it was three days that a picture was able to see stay in people's minds.
So we decided that we need to figure out what does stay in people's minds what works, what makes them care about the crisis or the issue longer
So we did a study and what works and what doesn't. So we call people and we ask them questions and you say, which one of these does this increase or decrease your affinity towards the crisis.
So if someone asks you, refugees undergo stringent security checks. Would that make you feel better about refugees or worse and take a second and think what you think most people would feel
This question surprised me. It actually decreased people's affinity for the cause, because it reminded them that there are security issues.
Refugees more likely to graduate college and secure jobs than non refugees.
Is that going to increase or decrease people support in this case that decreased Democratic support but increased Republican support.
Nearly half of refugees or children.
Increased all support Albert Einstein was a refugee think it increased or decreased support it increased all support. We did a test on Catholics.
60 million to displace 21 million refugees are fleeing their homes and then we had another one that said, Jesus was a refugee, the numbers had no change in support. Jesus was a refugee had an large increase
What this told us is that if you say there is a global refugee crisis. They're 16 million displaced. No one cares right these are fleeting numbers. They mean nothing to us and it goes right out of our head.
But if we can take the numbers and have them say this is the way it impacts you and what you care about people will become donors, people are affected, and they will remember
So we did a campaign during the major league baseball playoffs for each team that you would be watching, we put up their stadium and the number of people.
42,000 new people flee their homes on a daily basis. This is the same size as the crowd that you are watching on TV and this case at the Minute Maid Park, home of the Houston Astros
It let us take this big number 42,000 people flee their home every day. That means nothing to anyone.
And give it a visual from something that people care about, they are watching this stadium, they are watching this crowd. They can see all these people
And it meant something it increase donations by 100,000 recurring donors. People, who keep giving
If you are able to tell good numerical stories. If you're able to not just take the data analysis, but actually show the why, why we should care about it and make it mean something to people.
Then, whether it's for engagement action donation customers whatever you're trying to do. If those numbers can impact someone personally, you can have a lasting decision for them.
Really overall what this says is, we come back to data ism and humanism, you know, data gives us incredible insights to things. We'd never be able to see, but we need to retain our own experience and our own tuition.
And make sure that we were always asking the right questions of the data. The who the what the how and the, why should we care. And if we're able to do that then we really can deliver true value in both our personal and working moms. Thank you.

285. Andrew Knight. 00:52:52.410 -> 00:53:20.430

Thank you very much liberty for sharing such a helpful way of thinking about numbers and I was wondering, in a couple minutes that we have. If you have any
Like rules of thumb or your risks that you use when you're reading a news article or, you know, the latest piece of information that comes out about coven. Is there anything that that you do consciously to try to make your way through some of the distortion of numbers.

287. Paul McCartney. 00:53:21.450 -> 00:54:16.380

I think the first and most important thing is to not believe any of it right off the bat, and I hate saying that because you don't want to be this.
You know, sort of non believer, but you really can't believe it right off the bat, the biggest numbers. The 95% accurate or the double the risk of death. That's what makes you read the article.
I will say, I think the headlines are almost always way worse than the article itself. A lot of times the article itself will have the caveats, and will have the the world. Well, if you look at it this way, the headlines never will.
Because the headlines are really about getting people to read it. And in general, like I know for example. Whenever I write, I don't get to choose the headline. I have zero SEO for it.
And it will be what is going to get people to read it and it's not going to have the well it's not a double a risk of death of you dying from French fries. It's actually you know headline. Oh, a 1% increase in risk of death. It just doesn't get you to read it.

292. Andrew Knight. 00:54:18.180 -> 00:54:43.470

So it just takes like scrutinizing i mean i think i dropped something in the chat earlier that, you know, it gives me a headache sometimes to try to make my way through, what is the actual
risk level that's being presented and of course the always important question that john has dropped in the in the chat to leave us with and maybe you could leave this as a cliffhanger. How do we win the lottery.

294. Paul McCartney. 00:54:43.830 -> 00:54:44.760

Got to buy the book.

295. Andrew Knight. 00:54:45.510 -> 00:54:46.170

gotta buy the

296. Paul McCartney. 00:54:46.200 -> 00:55:02.100

Book. I'm kidding, I'm kidding.
I actually deleted that example because I was running out of time. Get Away With Murder, you gotta go look at OJ it and it worked out. Sure. You really want to try that. But there's, there's a lot of questions about statistical change of samples.

298. Andrew Knight. 00:55:03.360 -> 00:55:25.380

Well, thank you again so much for taking time to share these thoughts with us. And folks, if you can go down to the bottom of your zoom window, you'll see a
button that says reactions. And so if you'd click that reactions button and let's give liberty, a virtual you know Round of applause for sharing her expertise with us.

